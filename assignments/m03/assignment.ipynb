{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fca3c56e",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "# Defining network data\n",
    "\n",
    "Networks appear virtually every corner of the world. But these networks may not explicitly appear in data.\n",
    "Thus, defining a network from data is an essential yet often overlooked aspect of network analysis. In this notebook, we will explore non-network data and create a network dataset. This will help us understand the complex and crucial decisions required to initiate network analysis.\n",
    "\n",
    "## Data\n",
    "\n",
    "Our dataset is taken from [Copenhagen Network study](https://www.nature.com/articles/s41597-019-0325-x), in which the study collected data about physical proximities of about 700 students measured based on Bluetooth signals with smartphone. Our dataset is a subset of the original dataset, and I added random student names to make the dataset closer to raw form. The original data can be obtained from [here](https://figshare.com/articles/dataset/The_Copenhagen_Networks_Study_interaction_data/7267433/1?file=14000795).\n",
    "\n",
    "## Defining your goal\n",
    "\n",
    "There are tons of networks that can be created from the same data, and the choice depends on the specific research question. Thus, it is crucial to have a well-thought-out plan in place before touching the data.\n",
    "\n",
    "Let's suppose that we want to use the data to identify who to vaccinate to prevent communicable disease from spreading among the students. A communicable disease spreads through contacts with close physical proximity. This means that we want a physical contact network of the students.\n",
    "\n",
    "## Understanding data\n",
    "\n",
    "Let us focus on our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65c054e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "filename = \"proximity_data.csv\"\n",
    "contact_data_table = pd.read_csv(filename)\n",
    "contact_data_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f564cda",
   "metadata": {},
   "source": [
    "While you may have an idea about what the columns represent, I encourage to read the README carefully whenever available. Misunderstanding data format and semantics are a common mistake, and it becomes a disastrous as the analysis moves forward.\n",
    "\n",
    "The README attached to the data is the following:\n",
    "```raw\n",
    "column names:\n",
    "\t- timestamp\n",
    "\t- user A\n",
    "\t- user B\n",
    "\t- received signal strength\n",
    "\n",
    "Notes:\n",
    "Empty scans are marked with user B = -1 and RSSI = 0\n",
    "Scans of devices outside of the experiment are marked with user B = -2. All non-experiment devices are given the same ID.\n",
    "```\n",
    "\n",
    "## Inclusion criteria\n",
    "\n",
    "Now, let's define the network of the students.\n",
    "The raw data may contain errors, and not all Bluetooth signal data observed can be considered as close physical interactions. Therefore, it is necessary to filter out certain observations. To do so, we must establish clear inclusion criteria to determine what should be considered as physical interactions.\n",
    "\n",
    "Here, our inclusion criteria are the following\n",
    "1. Bluetooth signals must be stronger than -75dB.\n",
    "2. Focus on the interactions between students who participated in the experiment.\n",
    "3. Ignore the empty scans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954343a3",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# Assignment:\n",
    "# Filter out the interactions.\n",
    "#\n",
    "# Hint: pandas.DataFrame.query is a conveient API for filtering rows based on the column values.\n",
    "#\n",
    "# data_table =\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d26f53",
   "metadata": {},
   "source": [
    "## Normalization\n",
    "\n",
    "Now, let's create the network data from the filtered proximity data. The firs step is the *normalization* (or make the data tidy, which we will cover in the next module).\n",
    "The normalization in context of network data means to reduce redundancy in the data, and make it easier for subsequent network analysis.\n",
    "This includes\n",
    "1. Assigning IDs to nodes and create a table for nodes `node_table`\n",
    "2. Creating `edge_table`. Each row consists of two node IDs forming an edge.\n",
    "\n",
    "Let's create the node table. An easiest and fastest way is to use `numpy.unique` with flag 'return_inverse=True'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81140457",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assignment:\n",
    "# Create the node table (in pandas DataFrame) with columns, `node_id`, and `student_name`. For example,\n",
    "#\n",
    "# | node_id | student_name   |\n",
    "# | 0       | Charlotte Bell |\n",
    "# | 1       | Anna Volkova   |\n",
    "#  .....\n",
    "#\n",
    "# Hint:\n",
    "# numpy.unique is useful in assigning unique IDs.\n",
    "# For instance, consider an array of names\n",
    "# > names = [\"Bob\", \"Alice\", \"Bob\", \"James\", \"James\", \"Hana\"]\n",
    "# With `numpy.unique`, we can generate unique IDs for the names\n",
    "# > unique_names, name_ids = numpy.unique(names, return_inverse=True)\n",
    "# where unique names is a list of unique names, and name_ids is a represention of the input array but with integer IDs, instead of the name strings.\n",
    "# Don't forget to flag up `return_inverse=True` otherwise you'll get only the unique_names.\n",
    "#\n",
    "# The list of user names can be generated by\n",
    "# > user_names = data_table[[\"user_a\", \"user_b\"]].values.reshape(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c292289",
   "metadata": {},
   "source": [
    "Next, let's create the edge table. Since two users can have duplicated edges, we need to count the number of edges between the users.\n",
    "\n",
    "First, let's forget about the duplicated edges and create the edge table, with `src` and `trg`. We consider the network to be undirected, and thus it doesn't matter whether `user_a` is the `src` or `trg`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0933642",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assignment:\n",
    "# Create the edge  table (in pandas DataFrame) with columns, `src`, Note that `trg`. `src` should be smaller or equal to `trg`.\n",
    "# For example,\n",
    "#\n",
    "#  | src | trg |\n",
    "#  | 0   | 1   |\n",
    "#  | 0   | 22  |\n",
    "#  .....\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9bde533",
   "metadata": {},
   "source": [
    "Finding the duplicates in the edge table is a common but daunting task (you will find it soon).\n",
    "\n",
    "Since we consider the network to be undirected, we want to group rows with the same ID pairs (order insensitive, e.g., (1,2) and (2,1) represent the same pair).\n",
    "The easiest but inefficient way is to use `pandas.groupby` API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c98153a",
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_edge_table = (\n",
    "    edge_table.groupby([\"src\", \"trg\"]).size().reset_index(name=\"weight\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51dc554",
   "metadata": {},
   "source": [
    "Alternatively, you can represent an edge with a single complex number. A complex number consists of two values, the real part and the imaginary part.\n",
    "We can create a complex number from two node Ids. For instance, for an edge connecting nodes 10 and 20, we can represent it by a complex number\n",
    "$$\n",
    "10 + 20 j\n",
    "$$\n",
    "With this representation, we can find duplicates and compute the frequencies more efficiently. For instance,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842cbee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_trg = edge_table[\"src\"].values + 1j * edge_table[\"trg\"].values  # pairing\n",
    "uvalues, edge_ids, counts = np.unique(\n",
    "    src_trg, return_counts=True, return_inverse=True\n",
    ")  # find the unique values and compute the frequency\n",
    "src, trg = np.real(uvalues[edge_ids]).astype(int), np.imag(uvalues[edge_ids]).astype(\n",
    "    int\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891f7530",
   "metadata": {},
   "source": [
    "Finally, save the data into the files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20cdfa52",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# Assignment:\n",
    "#\n",
    "# Create the \"node_table.csv\" and \"edge_table.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f95054",
   "metadata": {},
   "source": [
    "## Documentation\n",
    "\n",
    "It is highly recommended to thoroughly document the inclusion criteria and the code used to generate the data in order to ensure reproducibility. Additionally, comprehensive documentation makes you *replaceable*, meaning that if someone wishes to repeat or improve upon the process, the document explains all the steps taken on your behalf, so that you can focus on matters that interest you most now.\n",
    "\n",
    "```markdown\n",
    "# Assignment:\n",
    "# Write the documentation about the steps taken to compile the dataset. Make sure the following points:\n",
    "# 1. Data source: Which source data is your dataset compiled from?\n",
    "# 2. What is the inclusion criteria of your dataset? What data records are excluded?\n",
    "# 3. Format of your dataset. If it is table, explain each column.\n",
    "# 4. When do you compile your dataset?\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad1ebda",
   "metadata": {},
   "source": [
    "Now, you created the network data by your own. It is mobile, shareable, and reproduceable! Let's see how the network looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d733a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import igraph\n",
    "\n",
    "g = igraph.Graph.DataFrame(\n",
    "    edge_table[[\"src\", \"trg\"]],\n",
    "    directed=False,\n",
    ")\n",
    "igraph.plot(g, vertex_size=5, edge_width=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f491eb91",
   "metadata": {},
   "source": [
    "## Assignment\n",
    "\n",
    "1. Your final assignment is to create a dataset of a network of characters in Les Miserable by Victor Hugo.\n",
    "2. Obtain the source data from [http://ftp.cs.stanford.edu/pub/sgb/jean.dat](http://ftp.cs.stanford.edu/pub/sgb/jean.dat).\n",
    "This file consists of two tables, separated by a blank line.\n",
    "The first table is about the characters in the book, separated by space (or tab).\n",
    "The second table contains the section numbers and the characters appearing in each section, separated by \":\".\n",
    "3.\n",
    "4. Create the node table, containing the IDs of the characters and their names (in two letters).\n",
    "5. Create the edge table, consisting of \"src\" and \"trg\" columns representing node Ids.\n",
    "6. Visualize the network and write the documentation about your Les Miserable network dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
